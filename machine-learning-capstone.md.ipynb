{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanodegree Engenheiro de Machine Learning\n",
    "## Projeto final\n",
    "Bruno Aurélio Rôzza de Moura Campos\n",
    "\n",
    "10 de fevereiro de 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definição\n",
    "### 1.1 Visão geral do projeto\n",
    "Quando ocorre um acidente de carro, as pessoas mantem o foco em família, amigos e outros entes queridos. A parte burocrática de entrar em contato com um agente de seguros é o último lugar que alguem vai gastar esforços. Para minimizar esforços na utilização de um serviço de sinistro será feito uma análise preditiva e assim, garatir a menor perda de tempo possível ao usuário e fornecer assistência direcionada para melhor atender.<br/>\n",
    "\n",
    "\n",
    "### 1.2 Descrição do problema\n",
    "Para este tipo de problema será explorado uma variedade de métodos de _machine learning_ para encontrar o melhor para prever o custo de reparo (ou perda) para uma determinada reivindicação de seguro.<br/>\n",
    "\n",
    "Há um artigo científico [Allstate Insurance Claims Severity: A Machine Learning Approach](https://pdfs.semanticscholar.org/e513/fcdb0cd06515858e42bd82d9dfab14c97b45.pdf) que aborda este mesmo problema e utiliza as seguintes técnicas de _machine learning_:\n",
    "- regressão linear\n",
    "- _Random Forest_\n",
    "- _Gradient Boosted Trees_\n",
    "- _Suport Vectorial Machine_\n",
    "\n",
    "Como conclusão do artigo, é visto que o _Gradient Boosted Trees_ apresenta a melhor performace em comparação aos outros modelos.\n",
    "\n",
    "Um problema muito similar já apareceu no kaggle, na competição [Porto Seguro’s Safe Driver Prediction\n",
    "](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) onde o objetivo era construir um modelo probabilistico para inferir uma reinvindicação para o uso do seguro. <br/>\n",
    "Este projeto tem bastante similaridade com a competição do Porto Seguro, então é possivel dizer que a resolução deste problema será voltado a automatizar a previsão de custo e análise da gravidade de um sinistro. Como resultado, será possível garantir uma experiência mais eficiente aos clientes que necessitam. A seguradora que esta realizando o desafio, Allstate, forneceu os seguintes dados:\n",
    "<br/>\n",
    "1. Variáveis em train.csv e test.csv:\n",
    " - **id**: o id de um par de perguntas do conjunto de treinamento\n",
    " - **cat1** até **cat116**: variáveis de categoria (o intervalo de valores não é fornecido, nem os nomes das colunas).\n",
    " - **cont1** até **cont14**: variáveis contínuas (o intervalo de valores não é fornecido, nem os nomes das colunas).\n",
    " - **loss**: o valor que a empresa tem que pagar por uma determinada reivindicação. Esta é a variável de destino. - Em test.csv, a perda não está presente, já que vamos prever isso.\n",
    "\n",
    "2. Em train.csv:\n",
    " - Número de linhas = 188318\n",
    " - Número de colunas = 132\n",
    "\n",
    "3. Em test.csv:\n",
    " - Número de linhas = 125546\n",
    " - Número de colunas = 131\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "O problema é criar um algoritmo que prevê com precisão a gravidade das reivindicações. Como entrada, será recebido diferentes variáveis que os agentes examinam para decidir o status das reivindicações. Eles podem ser contínuos ou discretos. Como a variável de destino é uma quantidade contínua (o valor a ser pago ao cliente), é essencialmente uma função de regressão. Desta forma, analisando o objetivo da seguradora neste desafio, é possível notar o algoritmo resultante deverá fazer recomendações para os usuários.\n",
    "\n",
    "\n",
    "### 1.3 Métricas\n",
    "Há várias maneiras de avaliar este problema. Como a avaliação oficial deste projeto é feita pelo Kaggle usando erro absoluto médio (Mean absolute error - MAE), o mesmo será usado para avaliação de modelos entre a predição de perda e a perda atual reinvidicada, conforme consta nos datasets.\n",
    " Abaixo há a fórmula do MAE:\n",
    "\n",
    "<img src=\"images/mae.png\" />\n",
    "\n",
    "- _y_: valor True\n",
    "- _ŷ_: valor predito\n",
    "- n samples: número de exemplos estimados \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Análise\n",
    "### 2.1 Exploração dos dados\n",
    " Neste projeto foi utilizado a base de dados que a seguradora Allstate forneceu na competição do Kaggle contendo diversas informações técnicas (features).<br/>\n",
    " O conjunto de dados contém 2 arquivos .csv com informações necessárias para fazer uma previsão. Ao contatenar estes arquivos obtemos em torno de 318.5 MB com 132 colunas e 313864 linhas. Eles contem as seguintes features:<br/>\n",
    "1. Features em train.csv e test.csv:\n",
    " - **id**: o id de um par de perguntas do conjunto de treinamento\n",
    " - **cat1** até **cat116**: variáveis de categoria (o intervalo de valores não é fornecido, nem os nomes das colunas).\n",
    " - **cont1** até **cont14**: variáveis contínuas (o intervalo de valores não é fornecido, nem os nomes das colunas).\n",
    " - **loss**: o valor que a empresa tem que pagar por uma determinada reivindicação. Esta é a variável de destino. - Em test.csv, a perda não está presente, já que vamos prever isso.\n",
    "\n",
    "Cabe observar que em test.csv a coluna loss não esta presente pois é o que será predito.<br/>\n",
    "\n",
    "#### Análise dos dados continuos\n",
    "Segue abaixo uma análise estatísticas dos dados de _features_ continuas:\n",
    "\n",
    "<img src=\"images/describe-continuos.png\" />\n",
    "<img src=\"images/describe-continuos2.png\" />\n",
    "\n",
    "\n",
    "É possível notar que as variáveis contínuas estão distribuídas com a mesma quantidade 313864 e que o valor médio gira em torno de 0.49.<br/>\n",
    "\n",
    "Cabe destacar a variável que é o _target_ deste projeto `loss`. É notável que esta variável apresenta bem menos quantidade do que o total da distribuição (188318). Este fato é devido ao _merge_ dos dados fornecidos, feito para explorar e entender melhor o que foi fornecido.<br/>\n",
    "Um dado valioso nesta exploração é ver que a média de _loss_ é 3037.337686 <br/>\n",
    "O menor sinistro (_loss_) foi de 0.670000 <br/>\n",
    "O maior sinistro (_loss_) foi de 121012.250000\n",
    "- OBS: a variável `id` não tem relevância para este problema mas será utilizada para gerar a tabela com os resultados dos testes.\n",
    "<br/>\n",
    "\n",
    "#### Análise dos dados continuos\n",
    "Na próxima imagem há uma análise estatísticas dos dados de _features_ categóricas:\n",
    "\n",
    "<img src=\"images/describe-cat.png\"height=\"90%\" width=\"90%\"/>\n",
    "\n",
    "\n",
    "É possível notar que as variáveis categóricas apresentam uma quantidade de 313864 e muitas dessas apresentam o valor `A` como a moda na distribuição.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "Você pode também mostrar o atributo skeweness, para mostrar se sua distribuição está inclinada ou se respeita uma distribuição normal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 Visualização exploratória\n",
    "Nesta seção, você precisará fornecer alguma forma de visualização que sintetize ou evidencie uma característica ou atributo relevante sobre os dados. A visualização deve sustentar adequadamente os dados utilizados. Discuta por que essa visualização foi escolhida e por que é relevante. Questões para se perguntar ao escrever esta seção:\n",
    "- _Você visualizou uma característica ou um atributo relevante acerca do conjunto de dados ou dados de entrada?_\n",
    "- _A visualização foi completamente analisada e discutida?_\n",
    "- _Se um gráfico foi fornecido, os eixos, títulos e dados foram claramente definidos?_\n",
    "\n",
    "\n",
    "Ao analisar os dados fornecidos, observamos que que temos os seguites tipos de dados:\n",
    "- int64(1) = id\n",
    "- float64(15) = features continuas + loss\n",
    "- object(116) = features categóricas\n",
    "\n",
    "As imagens 1 e 2 são amostras dos dado de train e test.\n",
    "\n",
    "<img src=\"images/train_data.png\" height=\"70%\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "Imagem 1. Amostra do arquivo de train.csv\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/test_data.png\" height=\"70%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 2. Amostra do arquivo de test.csv\n",
    "\n",
    "\n",
    "Foi analisado a distribuição dos dados de duas formas:\n",
    "- PCA\n",
    "\n",
    "<img src=\"images/test_data.png\" height=\"70%\" width=\"60%\"/>\n",
    "\n",
    "#### PCA\n",
    "\n",
    "É visto que os dados estão bem agrupados e apresentaram num PCA de 3 dimensões para cada um de seus componentes a variância de 5.97%, 4.73% e 4.29%, sendo um total de 14,99 %.\n",
    "\n",
    "<img src=\"images/pca.png\"/>\n",
    "\n",
    "A imagem e mais as informações de variância mostram que os dados estão bem misturados, lenvando a concluir que as _features_ tem o mesmo peso nesta base de dados\n",
    "\n",
    "#### Distribuição Normal\n",
    "\n",
    "<img src=\"images/distribuicao.png\"/>\n",
    "\n",
    "A distribuição gaussiana mostra que as _features_ 4, 5, 8, 14 e loss estão um pouco mais divergentes do que é definido uma distribuição gaussiana.<br/>\n",
    "Para garantir uma distribuição mais padronizada, será necessário aplicar log. Na seção 3.1 Pré-processamento de dados é feito isto.\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Algoritmos e técnicas\n",
    "A relação entre as _features_ de 130 (116 + 14) com a variável _loss_ é fundamentqal para compreensão do problema. Para isso, a solução devenvolvida utiliza a técnica de _Hybrid filtering_, para melhorar os resultados do sistema. Mais especificamente, o sistema utiliza o método de _ Feature-combination_  para determinar a similaridade das _features_.<br/>\n",
    "Também uma técnica fundamental para o endentimento do preblema é analisar as correlações entre as _features_ para selecionar os melhores resultados. \n",
    "Na parte de exploração de dados foi convertido valores categóricos de alfabetos em números que podem ser mais fáceis de serem processados. <br/>\n",
    " Os modelos testados foram: \n",
    " \n",
    " - **Regressão linear:** É o algoritmo estatístico no qual consiste em expressar a saída desejada na forma de função linear, onde cada instancia é relacionada com um peso. Sua principal vantagem é a simplicidade, sendo reconhecido como o algotimo mais simples de trabalhar com dados numéricos, cuntudo já as suas desvantagens consistem em trabalhar apenas com dados numéricos e sua eficiência em dados não lineares é baixa.\n",
    " \n",
    " \n",
    " - **XGBoost:** O XGBoost é uma biblioteca de otimização de gradiente distribuída otimizada projetada para ser altamente eficiente, flexível e portátil. Ele implementa algoritmos de aprendizado de máquina sob a estrutura Gradient Boosting. O XGBoost fornece um reforço de árvore paralela (também conhecido como GBDT, GBM) que resolve muitos problemas de ciência de dados de maneira rápida e precisa. O mesmo código é executado em ambientes distribuídos principais (Hadoop, SGE, MPI) e pode resolver problemas além de bilhões de exemplos.\n",
    " Este algortimo de boosting tem regularização para evitar overfitting, garante um processamento paralelo e ainda pode ser altamente flexivel, o XGBoost permite aos usuários definir personalizar objetivos de otimização e critérios de avaliação. Alem disso, o gerenciando valores faltantes (‘missing values’) pois ele possue rotinas internas para lidar com valores faltantes.\n",
    " Em relação à poda (prune) de árvores: XGBoost faz splits até o max_depth especificado e, em seguida, começa a podar a árvore de trás pra frente e remove as divisões para além das quais não há ganho positivo.\n",
    " \n",
    " \n",
    " - **Random Forest (Bagging):** Uma algortimo de classificação capaz de executar classificação em várias classes de conjunto de dados. Esse algoritmo requer pouca preparação dos dados e tem um custo logarítmico para o treinamento. Contudo, pode criar sistemas altamente complexos que não generalizam bem e assim ficarem instaveis pois pequenas variações nos dados podem resultar na geração de árvores compeltamente diferentes\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "### 2.4 Benchmark\n",
    "Após esta preparação dos dados foi realizado testes com modelos de machine learning para verificar qual tem melhor performace usando a divisão do Kfold e, finalmente, calculado o o erro médio quadrático (MAE). <br/>\n",
    "\n",
    "Em relação ao modelo de referência escolhido, que é a melhor pontuação da competição  Allstate Insurance Claims Severity: A Machine Learning Approach, aparece com MAE de 1109.70772 a partir disso serão comparados os resultados deste projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Metodologia\n",
    "\n",
    "### 3.1 Pré-processamento de dados\n",
    "Foi necessário aplicar várias técnicas de processamento para preparar os dados. Abaixo segue a lista:<br/>\n",
    "- Primeiramente, foi verificado se não havia valores \"missing\" em cada coluna.\n",
    "- Em seguida, foi separar as features categoricas e features continuas, obtendo o seguinte resultado:<br/>\n",
    "    <br/>\n",
    "    Continuos Features: ['cont1', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9'] \n",
    "    <br/>\n",
    "    Categorical Features: ['cat1', 'cat10', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat11', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat2', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat3', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat4', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat5', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat6', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat7', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat8', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat9', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99']\n",
    "    <br/>\n",
    "    <br/>\n",
    "- Por fim, foi visualizado os valores únicos, e obtido algumas métricas:<br/>\n",
    "\n",
    "\n",
    "|métricas| unique_values |\n",
    "|:-------:|-------------:|\n",
    "|count    | 116.000000|\n",
    "|mean     |   9.818966|\n",
    "|std      |  33.666807|\n",
    "|min      |   2.000000|\n",
    "|25%      |   2.000000|\n",
    "|50%      |   2.000000|\n",
    "|75%      |   4.000000|\n",
    "|max      | 326.000000|\n",
    "\n",
    "\n",
    "- Transformação da feature loss: Foi aplicado _log_ na feature loss para garantir uma distribuição mais gaussiana. Abaixo, há as imagens com as distribuições:<br/>\n",
    "\n",
    "<img src=\"images/default_loss.png\" height=\"50%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 3. Distribuição dos dados recebidos.\n",
    "\n",
    "<img src=\"images/log_loss.png\" height=\"50%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 4. Distribuição dos dados com aplicação logarítmica.\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Implementação\n",
    "No primeiro momento foi realizado o merge dos data sets de traning e test afim de obter uma noção geral do que havia sido fornecido para o desafio. Isso facilitou a análise das features e o entendimento dos objetivos deste desafio.<br/>\n",
    "Em seguida foi feito um **_data cleanning_** para fazer as seguintes funções:\n",
    "- Checar valor ausentes\n",
    "- Separar _features_ categoricas e _features continuas_\n",
    "- Checar valor únicos\n",
    "\n",
    "Depois disso, foi realizado um **_data preprocessing_** para fazer as seguintes atividades:\n",
    "- Transformação da _fature_ loss para log _loss_\n",
    "- Conversão de valores com _string_ para valores numéricos\n",
    "\n",
    "Na sequencia, se viu necessidade de realizar um **_feature engineer_** com os seguintes passos:\n",
    "\n",
    "- Analisar graficamente a _feture loss_\n",
    "\n",
    "<img src=\"images/default_loss.png\" height=\"50%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 5. Distribuição da _feature_ loss.\n",
    "\n",
    "\n",
    "\n",
    "- Analisar graficamente as _feature_ contínuas\n",
    "\n",
    "<img src=\"images/corr_cont_types_1_ate_4.png\" height=\"50%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 6. correlação das _features_ contínuas entre loss, 1 e 4.\n",
    "\n",
    "<img src=\"images/corr_cont_types_5_ate_8.png\" height=\"50%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 7. correlação das _features_ contínuas entre loss, 5 e 8.\n",
    "\n",
    "<img src=\"images/corr_cont_types_13_ate_14.png\" height=\"50%\" width=\"60%\"/>\n",
    "\n",
    "Imagem 8. correlação das _features_ contínuas entre loss, 13 e 14.\n",
    "\n",
    "<img src=\"images/corr_cont_types_all.png\" height=\"100%\" width=\"100%\"/>\n",
    "\n",
    "Imagem 9. correlação de todas as _features_ contínuas e loss.\n",
    "\n",
    "\n",
    "\n",
    "- Analisar a matriz de correlação das _feature_ contínuas\n",
    "\n",
    "<img src=\"images/corr_matrix_cont.png\" height=\"100%\" width=\"100%\"/>\n",
    "\n",
    "Imagem 10. Distribuição dos dados recebidos.\n",
    "\n",
    "- Foi decido não analisar graficamente as _fetures_ categóricas pois havia muitas. Como mlehor sulução ainstao foi calculado e obtido uma lista com as 10 melhores correlações, conforme resultado abaixo:<br/>\n",
    "\n",
    "\n",
    "\n",
    "Depois, foi analisado como deve ser feito a submissao e preparado o arquivo para isto.<br/>\n",
    " Após esta fase de data engineer, foi feito um _Split train and test_ e preparado a função _Mean absolute error (MAE)_ e a divisão com _K-Folds Cross_.\n",
    "\n",
    "\n",
    "Para não repetir código, descidiu-se que uma função tranning seria útil nesta fase do projeto. Esta função faz as seguintes coisas:\n",
    "- Calcula tempo de processamento\n",
    "- Executa a predição em cada fold\n",
    "- Calcula o MAE\n",
    "\n",
    "Código da função de tranning:\n",
    "\n",
    "```\n",
    "def train_model(model, num_folds):\n",
    "    \"\"\"Function by Train model\"\"\"\n",
    "        \n",
    "    print(\"Begin training\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # declare a KFold instance\n",
    "    kfold = KFold(n_splits = num_folds, random_state = 10)\n",
    "    \n",
    "    # number of models\n",
    "    num_models = 1\n",
    "    \n",
    "    # array to store results after each fold\n",
    "    results = np.zeros((X_test.shape[0], k))\n",
    "        \n",
    "    # train K-1 Random Forests\n",
    "    for i, (train, val) in enumerate(kfold.split(X_train)):\n",
    "        # get smaller training set and create validation set\n",
    "        X_train_mini, X_val = X_train.iloc[train], X_train.iloc[val]\n",
    "        y_train_mini, y_val = y_train[train], y_train[val]\n",
    "\n",
    "        # train model\n",
    "        model.fit(X_train_mini, y_train_mini)\n",
    "\n",
    "        # make predictions \n",
    "        preds = model.predict(X_val)\n",
    "        \n",
    "        # absolute error\n",
    "        error = mean_absolute_error(np.exp(y_val) - shift, np.exp(preds) - shift)\n",
    "        print(\"MAE on fold {} is {}\".format(i, error))\n",
    "\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_predictions = np.exp(model.predict(X_test)) - shift\n",
    "        \n",
    "        # Sum predictions\n",
    "        results[:,i] = test_predictions\n",
    "        \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"\\nTraining done! Time Elapsed:\", end - start, \" seconds.\")\n",
    "\n",
    "    # Error over k folds\n",
    "    avg_error = np.mean(results)\n",
    "\n",
    "    return test_predictions\n",
    "```\n",
    "\n",
    "Ao finalizar a preparação dos dados foi feito um a aplicação dos modelos de machine learning que serão melhor detalhados na parte de refinamento.\n",
    "\n",
    "\n",
    "\n",
    "### 3.3 Refinamento\n",
    "\n",
    "Foi utilizado três modelos de machine learning:\n",
    "- Linear Regression\n",
    "  - A modelagem da regressão linear foi analisada com os dados normalizados e não normalizados e se chegou a conclusão que não há diferença no resultado final.\n",
    "  - O melhor resultado ocorreu com um erro absoluto médio de 1791.1369743102152\n",
    "  \n",
    "\n",
    "- Random Forest (Bagging)\n",
    " - Foi testado o número de estimadores para 20, 50 e 100 para encontrar a modelagem com melhor performace. Como análise de resultado observou-se que o aumento do número de estimadores melhorou o score.\n",
    "  - O melhor resultado ocorreu com um erro absoluto médio de 1854.7308563695506, sendo pior em relação a regressão linear.\n",
    "  - Um problema observado foi que com o aumento de estimadores o tempo de processamento tambem aumentou consideravelmente.\n",
    "  \n",
    "- XGBoost\n",
    " - Foi realizado três diferentes modelagens para o XGBoost\n",
    " - Primeiramente foi utilizado uma estrutura de dados interna do XGBoost para garantir o uso de memória mais eficiente e assim treinar mais rápido.\n",
    " - Para este algoritmo se viu a necessidade de criar um método específico para realizar as seguintes funcionalidades:\n",
    "  - calcular o tempo de processamento\n",
    "  - embaralhar os dados durante cada fold\n",
    "  - executar a predição\n",
    "  - armazenar a predição num array\n",
    "  - analisar a média de previsões sobre os folds\n",
    "  - Abaixo, segue a função:\n",
    "```\n",
    "  def train_test_xgboost(model, early_stopping_rounds):\n",
    "    kf = KFold(n_splits = k, shuffle = True, random_state = random_state)\n",
    "    results = np.zeros((X_test.shape[0], k))\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        print(\"Begin training and testing base model on fold {}\".format(i))\n",
    "        start = time.time()\n",
    "        \n",
    "        X_train_mini, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_mini, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # train model\n",
    "        model.fit(X_train_mini, \n",
    "                   y_train_mini, \n",
    "                   eval_metric = eval_error, \n",
    "                   eval_set = [(X_train_mini, y_train_mini), (X_val, y_val)], \n",
    "                   early_stopping_rounds = early_stopping_rounds,\n",
    "                   verbose = False)\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Training time elapsed on fold {} is {}\".format(i, end - start))\n",
    "        \n",
    "        # Predict on validation set \n",
    "        val_predictions = model.predict(X_val, ntree_limit = model.best_ntree_limit)\n",
    "        error = mean_absolute_error(np.exp(y_val) - shift, np.exp(val_predictions) - shift)\n",
    "        print(\"Error on fold {} is {} \\n\".format(i, error))\n",
    "                \n",
    "        # Predict on test set\n",
    "        test_predictions = np.exp(model.predict(X_test, ntree_limit = model.best_ntree_limit)) - shift\n",
    "        # Sum predictions\n",
    "        results[:,i] = test_predictions    \n",
    "\n",
    "    # Average predictions\n",
    "    mean_results = results.mean(axis = 1)\n",
    "    print(test_predictions.shape)\n",
    "    return mean_results\n",
    "```\n",
    "  - A escolha dos parametro de cada um dos três modelos foi crucial para um melhor resultado.\n",
    "  - Para reduzir o overfitting foi testado a profundidade da árvore com 5, 7 e 9 níveis.\n",
    "  - Para reduzir a perda ao mínimo necessário em cada leaf da árvore foi testatdo o _gamma_ com 0 e 1.\n",
    "  - Para garantir a soma mínima dos pesos das instancias foi testado o _min_child_weight_ com 5 e 6.\n",
    "  - Um fato importante para se considerar é que foi mantido o parâmetro de _learning_rate_ com um valor default de 0.1.\n",
    "  - Por fim, o melhor resultado ocorreu com um erro absoluto médio de 1753.6550894377651, sendo o melhor resultado do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Resultados\n",
    "\n",
    "### 4.1 Modelo de avaliação e validação\n",
    "Para avaliar cada algoritmo de machine learning testado foi utilizado o erro absoluto médio (MAE) com a seguinte implemetação:<br/>\n",
    "```\n",
    "# Custom eval metric\n",
    "def eval_error(preds, dtrain):\n",
    "    \"\"\"evaluation\"\"\"\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "```\n",
    "\n",
    "Foi feito um comparativo entre a perda prevista e a perda real para cada afirmação do conjunto de testes. Isso garantiu a escolha do algoritmo melhor performático dentre os testados.<br/>\n",
    "\n",
    "Já para validação dos dados foi utilizado _K-Folds Cross Validation_. O KFold divide todas as amostras em grupos de amostras, chamadas dobras de tamanhos iguais (se possível). A função de predição é aprendida usando dobras e a dobra deixada de fora é usada para teste. Foi implementado da seguinte forma:<br/>\n",
    "\n",
    "```\n",
    "# replicate the results\n",
    "random_state = 16\n",
    "\n",
    "# folds\n",
    "k = 5\n",
    "\n",
    "# declare a KFold instance\n",
    "kfold = KFold(n_splits = num_folds, random_state = 10)\n",
    "```\n",
    "\n",
    "Ao testar o algoritmo de XGBoost houve a necessidade de alterar a implementação, conforme abaixo:<br/>\n",
    "```\n",
    "kf = KFold(n_splits = k, shuffle = True, random_state = random_state)\n",
    "```\n",
    "\n",
    "Ao final do processo, quando o modelo iterar/treinar 5 vezes, é obtido um _score_ de como o modelo está generalizando. Tambem é possível notar que, esse processo faz com que o treino do modelo demore um pouco mais, mas é crucial para ter certeza de que o modelo esta generalizando bem.<br/>\n",
    "\n",
    "Ao utilizar a função de `train_model` é possivel garantir que o modelo irá generalizar bem e apresentar uma maior robustez para dados não visto.<br/>\n",
    "\n",
    "No desafio em questão foi notado pequenas alterações como resultado final conforme detalhado nos testes mais abaixo (seção 4.1.1). Cabe ressaltar a importancia da função  `train_model` para este projeto, o qual realiza o trainamento e validação  dos dados.\n",
    "\n",
    "#### 4.1.1 Resultado de cada modelo\n",
    "Abaixo segue os resultados do modelo de **regressão linear**: \n",
    "- Neste primeiro resultado foi testado sem os dados normalizados.\n",
    "\n",
    "<img src=\"images/result/linear_regression-1.png\"/>\n",
    "\n",
    "\n",
    "- Neste segundo caso foi testado com os dados normalizados:\n",
    "\n",
    "<img src=\"images/result/linear_regression-2.png\"/>\n",
    "\n",
    "\n",
    "Abaixo segue os resultados do modelo de **random forest**: \n",
    "- Neste caso foram usados os seguintes parâmetros:\n",
    "\n",
    "|Parâmetro       | Valor |\n",
    "|:-------:       |-------------:|\n",
    "|n_estimators    |20|\n",
    "|n_jobs          |-1|\n",
    "|verbose         |1|\n",
    "|max_depth       |30|\n",
    "\n",
    "<img src=\"images/result/random_forest-1.png\"/>\n",
    "\n",
    "\n",
    "- Neste caso foram usados os seguintes parâmetros:\n",
    "\n",
    "|Parâmetro       | Valor |\n",
    "|:-------:       |-------------:|\n",
    "|n_estimators    |50|\n",
    "|n_jobs          |-1|\n",
    "|verbose         |1|\n",
    "|max_depth       |30|\n",
    "  \n",
    "<img src=\"images/result/random_forest-2.png\"/>\n",
    "\n",
    "  \n",
    "- Neste caso foram usados os seguintes parâmetros:\n",
    "  \n",
    "|Parâmetro       | Valor |\n",
    "|:-------:       |-------------:|\n",
    "|n_estimators    |100|\n",
    "|n_jobs          |-1|\n",
    "|verbose         |1|\n",
    "|max_depth       |30|\n",
    "\n",
    "<img src=\"images/result/random_forest-3.png\"/>\n",
    "\n",
    "\n",
    "Abaixo segue os resultados do modelo de **XGBoot Regressor**: \n",
    "- Neste caso foram usados os seguintes parâmetros:\n",
    "\n",
    "|Parâmetro       | Valor |\n",
    "|:-------:       |-------------:|\n",
    "|learning_rate   |0.1|\n",
    "|n_estimators    |1000|\n",
    "|max_depth       |7|\n",
    "|min_child_weight|5|\n",
    "|gamma           |0|\n",
    "|subsample       |1|\n",
    "|colsample_bytree|1|\n",
    "|reg_alpha       |1|\n",
    "|silent          |True|\n",
    "|seed            |random_state|\n",
    "|nthread         |-1|\n",
    "\n",
    "<img src=\"images/result/xgboost-1.png\"/>\n",
    "\n",
    "\n",
    "- Neste caso foram usados os seguintes parâmetros:\n",
    "\n",
    "|Parâmetro       | Valor |\n",
    "|:-------:       |-------------:|\n",
    "|learning_rate   |0.1|\n",
    "|n_estimators    |1000|\n",
    "|max_depth       |5|\n",
    "|min_child_weight|6|\n",
    "|gamma           |1|\n",
    "|subsample       |1|\n",
    "|colsample_bytree|1|\n",
    "|reg_alpha       |1|\n",
    "|silent          |True|\n",
    "|seed            |random_state|\n",
    "|nthread         |-1|\n",
    "\n",
    "<img src=\"images/result/xgboost-2.png\"/>\n",
    "\n",
    "\n",
    "- Neste caso foram usados os seguintes parâmetros:\n",
    "\n",
    "|Parâmetro       | Valor |\n",
    "|:-------:       |-------------:|\n",
    "|learning_rate   |0.1|\n",
    "|n_estimators    |1000|\n",
    "|max_depth       |9|\n",
    "|min_child_weight|6|\n",
    "|gamma           |1|\n",
    "|subsample       |1|\n",
    "|colsample_bytree|0.5|\n",
    "|reg_alpha       |1|\n",
    "|silent          |True|\n",
    "|seed            |random_state|\n",
    "|nthread         |-1|\n",
    "\n",
    "<img src=\"images/result/xgboost-3.png\"/>\n",
    "\n",
    "\n",
    "### 4.2 Justificativa\n",
    "Após testar os três algoritmos, foi analisado qual obteve o menor erro absoluto médio. A conclusão que se chegou foi um MAE de 1753.6550894377651 para o XGBoost.\n",
    "<br/>\n",
    "Em relação ao modelo de referência escolhido, que é a melhor pontuação da competição para o conjunto de teste,  aparece com MAE de 1109.70772 com isso é possível avaliar que cabe melhorias futuras nas modelagem deste projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusão\n",
    "\n",
    "### 5.1 Forma livre de visualização\n",
    "Como foi verificado na seção anterior, os resultados foram muito bons. Na tabela abaixo segue a classificação quanto ao erro absoluto médio com cada respectivo modelo e o tempo de processamento (por dobra - _elapsed_ ) considerando o pior caso:\n",
    "\n",
    "|Modelo       | MAE | Tempo de processamento (seg)|\n",
    "|:-------:       |-------------:|-------------:|\n",
    "|random forest       |1854.7|29.8|\n",
    "|regressão linear    |1791.1|0.5|\n",
    "|xgboost             |1753.6|8.5|\n",
    "\n",
    "É possível notar que foi obtido um melhor resultado quando se utilizou mais processamento, no caso do modelo de _xgboot_.<br/>\n",
    "\n",
    "O que foi bem surpreendente e inesperado é o resultado da _random forest_. Inicialmente foi visto como um bom modelo para este problema, pois utilizam o _bagging_ de recursos de tal forma que cada _forest_ considera um subconjunto escolhido aleatoriamente dos dados disponíveis e ainda as saídas de cada uma das florestas são calculadas, resultando em uma saída suavizada que permite que a variância seja minimizada. Porém nos testes obteve um resultado pior que o modelo de regressão linear.\n",
    "\n",
    "### 5.2 Reflexão\n",
    " Durante todo o projeto foi documentato da melhor forma possível através de notebooks. Essencialmente, este projeto consistiu em uma comparação de modelos de regressão linear, random forest e xgboot para criar predições encima dos dados fornecidos.<br/>\n",
    " O projeto foi revisado para garantir a melhor qualidade possível. Foi seguido as orientações da Udacity para deixar o trabalho padronizado. <br/>\n",
    " A parte de visualização dos dados foi útil para tomar as decisões sobre as técnicas usadas. Todos os gráficos gerados foram salvos para poder utilizar neste documento e assim melhor explicar as suas importancias. <br/>\n",
    " Ao final do projeto foi possível criar previsões muito boas, graças a forma que os dados foram tratados. Isso facilitou muito a aplicação dos algoritmos de machine learning.\n",
    "\n",
    "\n",
    "### 5.3 Melhorias\n",
    "Uma possível melhoria para este caso de predição pode ser a utilização de redes neurais artificias trainadas para conseguir previsões mais refiandas e modelos mais potentes. Se houver a utilização de novas técnicas como redes neurais artificiais pode ser feito um comparativo com os modelos testados neste projeto.<br/>\n",
    "A solução final pode ser usada como refencia pois houve a utilização e justificativa de cada modelo encima dos dados fornecidos e obitdo o melhor resultado na comparação.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
